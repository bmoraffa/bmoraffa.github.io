<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Deep Dive into Diffusion Models</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #121212;
            color: #e0e0e0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        header {
            text-align: center;
            width: 100%;
            padding: 20px;
            margin-bottom: 10px;
        }
        h1 {
            font-size: 36px;
            margin-bottom: 10px;
            color: #fff;
        }
        .summary {
            font-size: 18px;
            color: #ccc;
            max-width: 800px;
            margin: 0 auto;
        }
        .meta-info {
            font-size: 14px;
            color: #bbb;
            margin: 10px auto;
            border-bottom: 1px solid #333;
            padding-bottom: 10px;
            display: flex;
            justify-content: space-between;
            max-width: 800px;
        }
        .section-divider {
            width: 100%;
            border-top: 1px solid #333;
            margin: 10px auto;
        }
        h2 {
            font-size: 28px;
            color: #fff;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        h3 {
            font-size: 24px;
            margin-bottom: 10px;
            color: #bbb;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
            padding-bottom: 100px; /* Adjusted padding to make space for footer */
        }
        .post {
            padding: 20px;
            margin-bottom: 20px;
        }
        .post p {
            font-size: 16px;
            line-height: 1.8;
            margin-bottom: 15px;
            text-align: justify;
        }
        .math {
            font-style: italic;
            font-family: serif;
        }
        .reference {
            margin-top: 20px;
        }
        .reference h2 {
            font-size: 24px;
            margin-bottom: 10px;
        }
        .reference p {
            font-size: 16px;
            line-height: 1.6;
            margin-bottom: 15px;
        }
        footer {
            background-color: #1f1f1f;
            color: #fff;
            text-align: right;
            padding: 10px 20px;
            width: calc(100% - 40px);
            z-index: 999;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-top: 2px solid #333;
        }
        footer .social-icons {
            display: flex;
            gap: 10px;
        }
        footer .social-icons a {
            color: #fff;
            text-decoration: none;
        }
        footer .social-icons img {
            width: 24px;
            height: 24px;
        }
        nav {
            position: fixed;
            top: 20px;
            left: 20px;
            width: 200px;
            background-color: #1f1f1f;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
            padding: 10px;
            box-sizing: border-box;
            border: 2px solid #333;
        }
        nav h2 {
            font-size: 20px;
            margin-top: 0;
            color: #fff;
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
        }
        nav ul {
            list-style: none;
            padding: 0;
        }
        nav ul li {
            margin-bottom: 10px;
        }
        nav ul li a {
            text-decoration: none;
            color: #e0e0e0;
            font-size: 16px;
            transition: color 0.3s;
        }
        nav ul li a.active,
        nav ul li a:hover {
            font-weight: bold;
            color: #1e90ff;
        }
    </style>
</head>
<body>
    <nav id="toc">
        <h2>Contents</h2>
        <ul>
            <li><a href="#forward-process">Forward Process</a></li>
            <li><a href="#reverse-process">Reverse Process</a></li>
            <li><a href="#training-model">Training the Model</a></li>
            <li><a href="#model-inference">Model Inference</a></li>
            <li><a href="#references">References</a></li>
        </ul>
    </nav>
    <div class="container">
        <header>
            <h1>Diffusion Models: Understanding Forward and Reverse Processes</h1>
            <div class="summary">
                This article provides an in-depth look at the mechanisms of forward and reverse processes in diffusion models, exploring how they are used to train and generate data effectively.
            </div>
            <div class="meta-info">
                <div>Author: Bahman Moraffah</div>
                <div>Estimated Reading Time: 20 min</div>
                <div>Published: 2021</div>
            </div>
        </header>

        <div class="post">
            <section id="forward-process">
                <h2>Forward Process</h2>
                <p>Recall that the continuous diffusion models transform data \( x \) drawn from a distribution \( p_\text{data}(x) \) through a variance-preserving Markov process known as forward process. The forward process introduces noise to the data over a sequence of continuous time steps \( \lambda \in [ \lambda_{\text{min}}, \lambda_{\text{max}}] \), \( \lambda_{\text{min}} < \lambda_{\text{max}} \),</p>
                <p class="math">\[
                   q(z_\lambda|x) = \mathcal{N}(\alpha_\lambda x, \sigma_\lambda^2 I),
                \]</p>
                <p>where \( \alpha_\lambda^2 = \frac{1}{1 + e^{-\lambda}} \) and \( \sigma_\lambda^2 = 1 - \alpha_\lambda^2 \). For intermediate values of \( \lambda \), i.e., transitioning from one noise level to a higher one, the transition is modeled as</p>
                <p class="math">\[
                   q(z_\lambda|z_{\lambda'}) = \mathcal{N}\left(\left(\frac{\alpha_\lambda}{\alpha_{\lambda'}}\right) z_{\lambda'}, \sigma_{\lambda|\lambda'}^2 I\right),
                \]</p>
                <p>where \( \sigma_{\lambda|\lambda'}^2 = (1 - e^{\lambda-\lambda'})\sigma_\lambda^2 \) and \( \lambda < \lambda' \), which means the forward process goes in the direction of decreasing \( \lambda \). Conditioning on \( x \), the forward process is</p>
                <p class="math">\[
                     q(z_{\lambda'}|z_{\lambda}, x) = \mathcal{N}(\mu_{\lambda'|\lambda}(z_{\lambda}, x), \sigma^2_{\lambda'|\lambda} I),
                \]</p>
                <p>where \( \mu_{\lambda'|\lambda}(z_{\lambda}, x) = e^{\lambda - \lambda'}\left(\frac{\alpha_{\lambda'}}{\alpha_{\lambda}} z_{\lambda} + (1 - e^{\lambda - \lambda'})\alpha_{\lambda'} x\right)\), and \( \sigma_{\lambda|\lambda'}^2 = (1 - e^{\lambda-\lambda'})\sigma_\lambda^2 \), decreasing \( \lambda \).</p>
            </section>
            <section id="reverse-process">
                <h2>Reverse Process</h2>
                <p>The reverse process is described by Gaussian transitions to generate samples progressively from standard Gaussian noise, \(p_\theta(z_{\lambda_\text{min}}) \sim \mathcal{N}(0,I)\), and uses neural networks to parameterize the reverse process,</p>
                <p class="math">\[
                p_\theta(z_{\lambda'}|z_\lambda) = \mathcal{N}(\tilde{\mu}_{\lambda'|\lambda}(z_\lambda, x_\theta(z_\lambda, \lambda)), (\tilde{\sigma}^2_{\lambda'|\lambda})^{1-\nu}(\sigma^2_{\lambda'|\lambda})^\nu),
                \]</p>
                <p>where \(x_\theta(z_\lambda, \lambda) = \frac{z_\lambda - \sigma_\lambda \epsilon_\theta(z_\lambda, \lambda)}{\alpha_\lambda}\) represents a neural network that estimate of the original data \(x\) given a particular noisy observation \(z_\lambda\), \( \epsilon_\theta(z_\lambda, \lambda) \) is another neural network or the same network with different outputs, predicting the noise component added at the diffusion step characterized by \( \lambda \) and \( \sigma_\lambda \) and \( \alpha_\lambda \) are coefficients that define how the noise scales and the data is attenuated during the forward process at noise level \( \lambda \). The variance \( \sigma^2_{\lambda'| \lambda}\) for the reverse transition from \(z_\lambda\) to \(z_{\lambda'}\), where \( \lambda' > \lambda\) indicates a move towards lower noise levels,  can be a function of the variances at each of these noise levels,</p>
                <p class="math">\[
                  \Sigma_{\lambda'| \lambda} = \exp((1-\nu) \log \tilde{\sigma}^2_{\lambda'|\lambda} + \nu \log \sigma^2_{\lambda'|\lambda}),
                \]</p>
                <p>where \( \Sigma_{\lambda'| \lambda}\) is the interpolated variance, as discussed in [2]. The hyperparameter \( \nu\) weights how much the direct step variance versus the target level variance contributes to the transition. It is worth emphasizing that during inference runs in an increasing sequence of \( \lambda\), from higher noise to low noise and clean data.</p>
            </section>
            <section id="training-model">
                <h2>Training the Model</h2>
                <p>To train this model, we minimize the discrepancy between the noise added in the forward process and the noise estimated by the model,</p>
                <p class="math">\[
                     \mathbb{E}_{\epsilon \sim \mathcal{N}(0, I), \lambda\sim p(\lambda)} \left[ \| \epsilon_{\theta}(z_{\lambda, \lambda}) - \epsilon \|_2^2 \right],
                \]</p>
                <p>where \( p(\lambda) \) is a distribution over \([\lambda_\text{min}, \lambda_\text{max} ]\),  \(\epsilon\) is the noise used in the forward process, \( \epsilon_{\theta}(z_{\lambda}, \lambda) \) is the model estimate of this noise, and \(z_\lambda = \alpha_\lambda x + \sigma_\lambda \epsilon\). This is a form of denoising score matching that helps the model learn to reverse the noise addition accurately.  The distribution of \( \lambda \) can affect the training dynamics and the focus on different noise levels. A specific scheduling, such as the cosine noise schedule, modulates \( \lambda \) values across the training and influences the distribution and variance of the noise levels encountered. Uniform distribution on \(\lambda\) results in the objective being proportional to the variational lower bound on the marginal log likelihood of the latent variable model.</p>
            </section>
            <section id="model-inference">
                <h2>Model Inference</h2>
                <p>Once trained, sampling from the model involves running the reverse process using the predicted noise values to progressively denoise the data. This process uses Langevin dynamics where samples are drawn progressively closer to the distribution of the original dataset \( p_\text{data}(x)\).</p>
            </section>
        </div>
        <section id="references" class="reference">
            <h2>References</h2>
            <p>[1] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in neural information processing systems 33 (2020): 6840-6851.</p>
            <p>[2] Nichol, Alexander Quinn, and Prafulla Dhariwal. "Improved denoising diffusion probabilistic models." In International conference on machine learning, pp. 8162-8171. PMLR, 2021.</p>
            <p>[3] Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. "Score-based generative modeling through stochastic differential equations." arXiv preprint arXiv:2011.13456 (2020).</p>
            <p>[4] Sohl-Dickstein, Jascha, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. "Deep unsupervised learning using nonequilibrium thermodynamics." In International conference on machine learning, pp. 2256-2265. PMLR, 2015. </p>
        </section>
    </div>
    <footer>
        <div>&copy; BM's Blog. All Rights Reserved.</div>
        <div class="social-icons">
            <!-- Add your social media icons and links here -->
        </div>
    </footer>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            const sections = document.querySelectorAll("section");
            const tocLinks = document.querySelectorAll("#toc a");

            function onScroll() {
                let currentSection = "";
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    if (pageYOffset >= sectionTop - 50) {
                        currentSection = section.getAttribute("id");
                    }
                });

                tocLinks.forEach(link => {
                    link.classList.remove("active");
                    if (link.getAttribute("href") === `#${currentSection}`) {
                        link.classList.add("active");
                    }
                });
            }

            window.addEventListener("scroll", onScroll);
        });
    </script>
</body>
</html>
