<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Bahman Moraffah - Books</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Bahman Moraffah</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="tut.html">Tutorials</a></div>
<div class="menu-item"><a href="mybook.html" class="current">Books</a></div>
<div class="menu-item"><a href="bio.html">Awards</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="presentation.html">Presentations</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-category">How to do Research?</div>
<div class="menu-item"><a href="courses.html">What&nbsp;to&nbsp;Study?</a></div>
<div class="menu-item"><a href="htdr.html">How&nbsp;to&nbsp;Do&nbsp;Research?</a></div>
<div class="menu-category">Links</div>
<div class="menu-item"><a href="https://github.com/bmoraffa">GitHub</a></div>
<div class="menu-item"><a href="https://scholar.google.com Google Scholar">Google&nbsp;Scholar</a></div>
<div class="menu-item"><a href="https://twitter.com/explore">Twitter</a></div>
<div class="menu-category">Classes</div>
<div class="menu-item"><a href="BIM.html">BIM</a></div>
<div class="menu-item"><a href="EEE554.html">RST</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Bahman Moraffah - Books</h1>
<div id="subtitle"><a href="https://ecee.engineering.asu.edu" target=&ldquo;blank&rdquo;>School of Electerical, Computer, and Energy Engineering</a></div>
</div>
<h2>Probability, Statistics, and Random Processes with Applications in Learning Theory</h2>
<p><b>Bahman Moraffah, 2021</b><br />
Probability theory provides a principled, practical, mathematical approach for learning theory. This book provides the foundamental background for doing research in machine learning, engineering, and statistics. It provides a unified treatment of theratical and practical aspenct of probability theory. The treatment is comprehensive and self-contained with many applications, targeted at researchers and students in machine learning, engineering, and applied statistics.</p>
<p><b>Contents:</b><br />
<b> Part I: Probability Theory</b></p>
<ul>
<li><p>Probability Space</p>
</li>
<li><p>Continuous and Discrete Random Variables </p>
</li>
<li><p>Multiple Random Variables</p>
</li>
<li><p>Parametric Point Estimation </p>
</li>
<li><p>Probability Theory: Applications</p>
</li>
<li><p>Convergence and Asymptotic Behavior<br /></p>
</li>
</ul>
<p><b> Part II: Random Processes </b></p>
<ul>
<li><p>Introduction to Random Processes </p>
</li>
<li><p>Markov Processes</p>
</li>
<li><p>Poisson Processes<br /></p>
</li>
</ul>
<p><b> Part III: Applications in Learning Theory </b></p>
<ul>
<li><p>Probability and its Application in Machine Learning</p>
</li>
<li><p>Bootstrap and Monte Carlo Resampling Methods</p>
</li>
<li><p>Information Geometry and its Applications in Machine Learning</p>
</li>
</ul>
<h2>Bayesian Modeling and Inference: A Bayesian Approach to Machine Learning </h2>
<p><b>Bahman Moraffah, 2022</b><br />
It provides a comprehensive treatment of advanced learning thoery. It spans a wide range of statistical tools from frequntists to Bayesian. This book extensively emphesizes on the aadvances in machine learning learning theory from Bayesian perspective. This book can be used for an advanced graduate course in Bayesian statistics.<br />
<b>Contents:</b><br /></p>
<ul>
<li><p>Preliminary </p>
<ul>
<li><p>Measure theoretic definition </p>
</li>
<li><p>Stochastic convergences</p>
</li>
<li><p>Information theory </p>
</li>
<li><p>Group theory</p>
</li>
<li><p>Conceptual things</p>
</li>
<li><p>Exponential family </p>
</li>
<li><p>Gaussian </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p>Theory of Point Estimation</p>
<ul>
<li><p>Likelihood and first order methods such as MLE</p>
</li>
<li><p>Delta method</p>
</li>
<li><p>Asymptotic statistics </p>
</li>
<li><p>M and Z estimators</p>
</li>
<li><p>U statistics</p>
</li>
<li><p>Empirical processes</p>
</li>
<li><p>L-Stattiitcs </p>
</li>
<li><p>Parametric minimax theory </p>
</li>
<li><p>Expectation-minimization algorithm to compute </p>
</li></ul>
</li>
<li><p>Introduction to nonparametric Statistics </p>
<ul>
<li><p>Concentration of measure</p>
</li>
<li><p>Density estimation </p>
</li>
<li><p>Minimax theory </p>
</li></ul>
</li>
<li><p>Statistical Decision Theory: A frequentist Approach </p>
</li>
<li><p>Bootstrap</p>
</li>
<li><p>Information Geometry </p>
</li>
<li><p>Regression </p>
<ul>
<li><p>Linear regression </p>
</li></ul>
</li>
<li><p>Introduction to Bayesian Statistics  </p>
<ul>
<li><p>Bayesian statistics: introduction and example — Bayes rule, Exchangeability, posterior, inference, marginalization</p>
</li>
<li><p>Singel parameter estimation </p>
</li>
<li><p>Multi- parameter estimation </p>
</li>
<li><p>Choice of prior </p>
</li>
<li><p>Frequentist property of Bayesian modeling  </p>
</li></ul>
</li>
<li><p>Advanced Posterior Computation</p>
<ul>
<li><p>Markov chain Monte Carlo method </p>
</li>
<li><p>Gibbs sampling — efficient Gibbs sampling</p>
</li>
<li><p>Metropolis-Hastings algorithm</p>
</li>
<li><p>Building MC algorithm </p>
</li>
<li><p>Variational Bayes</p>
</li></ul>
</li>
<li><p>Hierarchical Modeling </p>
</li>
<li><p>Mixture Model</p>
</li>
<li><p>Robust inference</p>
</li>
<li><p>Bayesian Regression </p>
<ul>
<li><p>Linear Models</p>
</li>
<li><p>Generalized linear model </p>
</li>
<li><p>Linear regression — Bayesian linear model, Model selection — Bayesian comparison, Gibbs sampling and model averaging </p>
</li></ul>
</li>
<li><p>Bayesian Decision theory </p>
</li>
<li><p>Poisson Processes</p>
<ul>
<li><p>Poisson distribution and relationship to multinomial and binomial distribution </p>
</li>
<li><p>Definition of Poisson process </p>
</li>
<li><p>Campbell’s theorem </p>
</li></ul>
</li>
<li><p>Bayesian nonparametrics I</p>
<ul>
<li><p>Dirichlet process &ndash; gamma process</p>
</li>
<li><p>Hierarchical Dirichlet process</p>
</li>
<li><p>Dependent Dirichlet process</p>
</li>
<li><p>Two-parameter Poisson-Dirichlet process </p>
</li></ul>
</li>
<li><p>Bayesian nonparametrics II</p>
<ul>
<li><p>Beta processes, stick-breaking, and power law</p>
</li>
<li><p>Indian Buffett process</p>
</li>
<li><p>Hierarchical Indian buffet proceses</p>
</li></ul>
</li>
<li><p>Bayesian nonparametric III</p>
<ul>
<li><p>Completely random measure</p>
</li>
<li><p>Normalized random measure</p>
</li>
<li><p>Kingman paintbox</p>
</li>
<li><p>Feature allocation and paintbox</p>
</li>
<li><p>Truncated random measure </p>
</li></ul>
</li>
<li><p>Bayesian Nonparameteric IV </p>
<ul>
<li><p>Gaussian Processes</p>
</li>
<li><p>Learning Functions and Gaussian Processes</p>
</li>
<li><p>Bayesian Optimization </p>
</li></ul>
</li>
<li><p>Bayesian Network and Causal Inference</p>
<ul>
<li><p>Directed Graphical model and Inference</p>
</li>
<li><p>Causal inference</p>
</li></ul>
</li>
<li><p>Undirected graphical model</p>
<ul>
<li><p>Definitions </p>
</li>
<li><p>Inference on Undirected graphs</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2020-07-06 12:33:03 MST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
