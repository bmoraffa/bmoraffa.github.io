<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Latent Diffusion Models</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #121212;
            color: #e0e0e0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        header {
            text-align: center;
            width: 100%;
            padding: 20px;
            margin-bottom: 10px;
        }
        h1 {
            font-size: 36px;
            margin-bottom: 10px;
            color: #fff;
        }
        .summary {
            font-size: 18px;
            color: #ccc;
            max-width: 800px;
            margin: 0 auto;
        }
        .meta-info {
            font-size: 14px;
            color: #bbb;
            margin: 10px auto;
            border-bottom: 1px solid #333;
            padding-bottom: 10px;
            display: flex;
            justify-content: space-between;
            max-width: 800px;
        }
        .section-divider {
            width: 100%;
            border-top: 1px solid #333;
            margin: 10px auto;
        }
        h2 {
            font-size: 28px;
            color: #fff;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        h3 {
            font-size: 24px;
            margin-bottom: 10px;
            color: #bbb;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
            padding-bottom: 100px;
        }
        .post {
            padding: 20px;
            margin-bottom: 20px;
        }
        .post p {
            font-size: 16px;
            line-height: 1.8;
            margin-bottom: 15px;
            text-align: justify;
        }
        .math {
            font-style: italic;
            font-family: serif;
        }
        pre code {
            background-color: #2d2d2d;
            color: #e0e0e0;
            display: block;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }
        nav {
            position: fixed;
            top: 20px;
            left: 20px;
            width: 200px;
            background-color: #1f1f1f;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);
            padding: 10px;
            box-sizing: border-box;
            border: 2px solid #333;
        }
        nav h2 {
            font-size: 20px;
            margin-top: 0;
            color: #fff;
            border-bottom: 2px solid #333;
            padding-bottom: 5px;
        }
        nav ul {
            list-style: none;
            padding: 0;
        }
        nav ul li {
            margin-bottom: 10px;
        }
        nav ul li a {
            text-decoration: none;
            color: #e0e0e0;
            font-size: 16px;
            transition: color 0.3s;
        }
        nav ul li a.active,
        nav ul li a:hover {
            font-weight: bold;
            color: #1e90ff;
        }
        footer {
            background-color: #1f1f1f;
            color: #fff;
            text-align: right;
            padding: 10px 20px;
            width: calc(100% - 40px);
            z-index: 999;
            display: flex;
            justify-content: space-between;
            align-items: center;
            border-top: 2px solid #333;
        }
        footer .social-icons {
            display: flex;
            gap: 10px;
        }
        footer .social-icons a {
            color: #fff;
            text-decoration: none;
        }
        footer .social-icons img {
            width: 24px;
            height: 24px;
        }
    </style>
</head>
<body>
    <nav id="toc">
        <h2>Contents</h2>
        <ul>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#overview">Overview</a></li>
            <li><a href="#training">Training</a></li>
            <li><a href="#algorithm">Conditional Generation with LDMs</a></li>
        </ul>
    </nav>
    <div class="container">
        <header>
            <h1>Latent Diffusion Models</h1>
            <div class="summary">
                Latent Diffusion Models (LDMs) extend diffusion models to a latent space, making the generative process more efficient and better suited for high-dimensional data.
            </div>
            <div class="section-divider"></div>
            <div class="meta-info">
                <div>Author: Bahman Moraffah</div>
                <div>Estimated Reading Time: 10 min</div>
                <div>Published: 2024</div>
            </div>
        </header>
        <!-- Article content -->
        <div class="post">
            <section id="introduction">
                <h2>Introduction</h2>
                <p>Latent Diffusion Models (LDMs) are a class of generative models that extend the idea of diffusion models to a latent space. Traditional diffusion models involve denoising a sample from a simple noise distribution (e.g., Gaussian) to produce a sample from the target data distribution, guided by a learned denoising function. LDMs apply this concept in a compressed latent space, leading to computational efficiency and better handling of high-dimensional data. Detailed discussions on LDMs can be found in Section <span class="math">\ref{sec:ldms}</span>.</p>
            </section>
            <section id="overview">
                <h2>Overview</h2>
                <p>The core idea of LDMs is to learn a mapping between the data space and a lower-dimensional latent space. Let \(X\) denote the data space and \(Z\) the latent space. The mapping consists of an encoder \(E: X \rightarrow Z\) and a decoder \(D: Z \rightarrow X\).</p>
                <p>The diffusion process in the latent space is defined by a forward process and a reverse process. The forward process gradually adds noise to a latent variable \(z_0\) to produce a sequence of increasingly noisy latents \(z_1, z_2, \ldots, z_T\). This is modeled by a Markov chain with transition probabilities \(q(z_{t+1}|z_t)\).</p>
                <p>The reverse process aims to denoise the latent variables back to the original signal. It is defined by a reverse Markov chain with transition probabilities \(p_\theta(z_{t-1}|z_t)\), where \(\theta\) are the parameters of the model learned during training.</p>
            </section>
            <section id="training">
                <h2>Training</h2>
                <p>The training objective for LDMs is to learn the parameters \(\theta\) that maximize the likelihood of the reverse process. This is typically done using a variational lower bound on the log-likelihood, leading to a loss function similar to that used in variational autoencoders (VAEs). The loss function can be decomposed into a reconstruction term and a KL-divergence term:</p>
                <p class="math">\[\mathcal{L}(\theta) = \mathbb{E}_{q(z|x)}[\log p_\theta(x|z)] - \beta \cdot D_{KL}(q(z|x) || p(z)),\]</p>
                <p>where \(x\) is a data sample, \(z\) is its latent representation, \(p_\theta(x|z)\) is the likelihood of the data given the latents, \(q(z|x)\) is the approximate posterior (encoder), \(p(z)\) is the prior over the latents, and \(\beta\) is a hyperparameter controlling the trade-off between the two terms.</p>
            </section>
            <section id="algorithm">
                <h2>Conditional Generation with LDMs</h2>
                <pre><code>
<ol>
<li><strong>Encoding:</strong> Encode data samples into latent space using an encoder \(E\):
    \[ z_0 = E(x) \]
</li>
<li><strong>Diffusion:</strong> Apply forward diffusion to obtain noisy latents \(z_1, z_2, \ldots, z_T\)</li>
<li><strong>Conditioned Denoising:</strong> Train reverse process \(p_\theta\) to denoise latents back to \(z_0\) considering condition \(c\):
    \[ z_{t-1} = p_\theta(z_{t-1}|z_t) \]
</li>
<li><strong>Decoding:</strong> Use decoder \(D\) to generate final samples from denoised latents:
    \[ \hat{x} = D(z_0) \]
</li>
<li><strong>Optimization:</strong> Update parameters \(\theta\) by minimizing loss \(\mathcal{L}(\theta)\) using gradient descent</li>
</ol>
                </code></pre>
            </section>
        </div>
    </div>
    <footer>
        <div>&copy; BM's Blog. All Rights Reserved.</div>
        <div class="social-icons">
            <a href="https://www.linkedin.com" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/c/ca/LinkedIn_logo_initials.png" alt="LinkedIn"></a>
            <a href="https://scholar.google.com" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/6/6b/Google_Scholar_logo.png" alt="Google Scholar"></a>
            <a href="https://github.com" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub"></a>
            <a href="https://twitter.com" target="_blank"><img src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Logo_of_Twitter.svg" alt="Twitter"></a>
        </div>
    </footer>
</body>
</html>
